---
title: "Course assignment for 'Statistical Computing' course"
author: "HC"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Course assignment for 'Statistical Computing' course}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Question

Use knitr to produce 3 examples in the book. The 1st example should contain texts and at least one figure. The 2nd example should contains texts and at least one table. The 3rd example should contain at least a couple of LaTeX formulas.


## answer

## 1st example

The 1st answer contains texts and at least one figure. 
```{r random, echo=TRUE}
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
plot(lm.D9)
```


## 2nd example

This is an Rmd program document that contains text and at least one table.

this program contains texts and at least one table.

The table is made using the data set included in R :Growth of Orange Trees, as follows:

```{r table, echo=TRUE}
knitr::kable(head(iris))
```


## 3rd example

This is an Rmd program document that contains a series of LaTeX formulas.

The formula is as follows (center):

$$e^{i\pi}+1=0$$

$$\int_0^\frac{2}{5} e^x {\rm d}x$$

$$\sum_{n=1}^\infty \frac{1}{n^2} =\frac{\pi ^2}{6}$$

$$\begin{bmatrix}
1 & 2 & 3 & 4 & 5\\
6 & 7 & 8 & 9 & 10\\
11 & 12 & 13 & 14 & 15
\end{bmatrix}$$

$$\begin{cases}
a_1x+b_1y+c_1z=d_1\\
a_2x+b_2y+c_2z=d_2\\
a_3x+b_3y+c_3z=d_3\\
\end{cases}
$$


## Question 3.3

The Pareto(a, b) distribution has cdf

$F(x)=1-(\frac{b}{x})^a,x\geq b>0,a>0.$

Derive the probability inverse transformation$F^{−1}(U)$ and use the inverse transform method to simulate a random sample from the Pareto(2, 2) distribution.Graph the density histogram of the sample with the Pareto(2, 2) density superimposed for comparison. 


## answer


The inverse function of F is $F^{-1}(y)=\frac{b}{(1-y)^{\frac{1}{a}}}$,its probability density function is $\frac{ab^a}{x^{(a+1)}}$
so The inverse function of Pareto(2,2) is $F^{-1}(y)=\frac{2}{(1-y)^{\frac{1}{2}}},0<=y<=1$.Its probability density function is$\frac{8}{x^3}$
The code and figure are shown below:

```{r}
n<-1000
uy<-runif(n)
ux<-2/((1-uy)^(1/2))#F(x)=1-(2/x)^2
hist(ux,prob=TRUE,main=expression(f(x)==8/x^3),freq=F,breaks=100)
y<-seq(0.01,50,0.1)
lines(y,8/y^3,col="red")
```



## Question 3.9

The rescaled Epanechnikov kernel [85] is a symmetric density function

$f_e(x)=\frac{3}{4}(1-x^2),|x|\leq 1.       (3.10)$

Devroye and Gy$\ddot{o}$rfi [71, p. 236] give the following algorithm for simulation
from this distribution. Generate iid $U_1, U_2, U_3\sim Uniform(−1, 1)$. If $|U_3| \geq |U_2|$ and$|U_3| \geq |U_1|$, deliver$U_2$; otherwise deliver$U_3$. Write a function to generate random variates from $f_e$, and construct the histogram density
estimate of a large simulated random sample.

## answer


The code and figure are shown below:

```{r}
n<-1000
u1<-runif(n,min=-1,max=1)#u1
dim(u1)<-c(1,n)
u2<-runif(n,min=-1,max=1)#u2
dim(u2)<-c(1,n)
u3<-runif(n,min=-1,max=1)#u3
dim(u3)<-c(1,n)
fy<-array(0,dim=c(1,n))
i=1
while (i<1001) {
   a=abs(u1[1,i])
   b=abs(u2[1,i])
   c=abs(u3[1,i])
   if ((c>=b) & (c>=a))
      fy[1,i]<-u2[1,i]
   else
      fy[1,i]<-u3[1,i]
   i<-i+1
}
hist(fy,prob=T,main=expression(f(x)==3/4(1-x^2)))
y<-seq(-1,1,0.01)
lines(y,3*(1-y^2)/4,col="red")
```


## Question 3.10
Prove that the algorithm given in Exercise 3.9 generates variates from the density $f_e$(3.10). 


## answer


Because $u_1,u_2,u_3\sim U(-1,1)$,so $x=|(u_1)|,y=|(u_2)|,z=|(u_3)|\sim U(0,1)$,and it is easy to see that $v=min \lbrace|(u_1)|,|(u_2)|\rbrace\sim 2(1-v),v\in(0,1)$.
If the variable generated by the algorithm is set as T, the complete event group given by the algorithm and the selection mode of the algorithm have the following relations:
$P(T<t)=P(T<t,|u_3|\geq|u_2|,|u_1|\leq|u_3|)+P(T<t,|u_3|\geq|u_2|,|u_1|>|u_3|)+P(T<t,|u_3|<|u_2|,|u_1|\leq|u_3|)$
$+P(T<t,|u_3|<|u_2|,|u_1|>|u_3|)$

$=P(u_2<t,|u_3|\geq|u_2|,|u_1|\leq|u_3|)+P(u_3<t,|u_3|\geq|u_2|,|u_1|>|u_3|)+P(u_3<t,|u_3|<|u_2|,|u_1|\leq|u_3|)$
$+P(u_3<t,|u_3|<|u_2|,|u_1|>|u_3|)$

$=P(u_2<t,|u_3|\geq|u_2|,|u_1|\leq|u_3|)+2P(u_3<t,|u_3|<|u_2|,|u_1|\leq|u_3|)$
$+P(u_3<t,|u_3|<|u_2|,|u_1|>|u_3|)$

$=P(u_2<t,|u_3|\geq|u_2|,|u_1|\leq|u_3|)+2P(u_3<t,|u_3|<|u_2|,|u_1|\leq|u_3|)$
$+P(u_3<t,min\lbrace|u_2|,|u_1|\rbrace>|u_3|)$

$=\int^{t}_{-1}{\frac{1}{2}}\int^{1}_{|u_2|}\int^{z}_{0}{dxdzdu_2}+2\int^{t}_{-1}{\frac{1}{2}}\int^{1}_{|u_3|}\int^{|u_3|}_{0}{dxdydu_3}$
$+\int^{t}_{-1}{\frac{1}{2}}\int^{1}_{|u_3|}{2(1-v)dvdu_3}$

$=\int^{t}_{-1}{\frac{1-{u_2}^2}{4}}{du_2}+\int^{t}_{-1}{|u_3|(1-|u_3|)}{du_3}+\int^{t}_{-1}{\frac{(1-|u_3|)^2}{2}}{du_3}$

Merge after expansion,it is obtained that:
$=\frac{1}{2}[\int^{t}_{-1}{\frac{1-{u_2}^2}{2}}{du_2}+\int^{t}_{-1}{1}{du_3}-\int^{t}_{-1}{{u_3}^2}{du_3}]$
$=-\frac{t^3}{4}+\frac{3t}{4}+\frac{1}{2}$
i.e. $P(T<t)=-\frac{t^3}{4}+\frac{3t}{4}+\frac{1}{2}$
After derivation, the probability density function of the variable T generated by this algorithm is $f(t)=\frac{3}{4}(1-t^2),|t|\leq1$
Therefore,it can be known that the variable T generated by this algorithm comes from the distribution $f_e$(3.10).

## Question 3.13

It can be shown that the mixture in Exercise 3.12 has a Pareto distribution
with cdf

$F(y)=1-(\frac{\beta}{\beta+y})^r,y \geq 0.$

(This is an alternative parameterization of the Pareto cdf given in Exercise3.3.) Generate 1000 random observations from the mixture with r = 4 and β = 2. Compare the empirical and theoretical (Pareto) distributions by graphing the density histogram of the sample and superimposing the Pareto density curve.


## answer

The inverse function of F(x) is $F^{-1}(y)=\frac{2}{(1-y)^{\frac{1}{4}}}-2,0 \leq y\leq 1$.Its probability density function is$\frac{64}{(2+x)^5}$
The code and figure are shown below:

```{r}
n<-1000
uy<-runif(n)
ux<-2/((1-uy)^(1/4))-2#F(x)=1-(2/y)^2
hist(ux,prob=TRUE,main=expression(f(x)==64/(2+x)^5),freq=F)
y<-seq(0,50,0.1)
lines(y,64/(2+y)^5,col="red")
```

## Question 5.1

Compute a Monte Carlo estimate of $\int_0^{\frac{\pi}{3}} sin(t)dt$ and compare your estimate with the exact value of the integral.

## answer

$\int_0^{\frac{\pi}{3}} sin(t)dt=\frac{\pi}{3}E(sin(t)),t\sim U(0,\frac{\pi}{3})$,the results and exact values of  Monte Carlo estimator are as follow:

```{r}
n<-1e5;
x<-runif(n,min=0,max=pi/3);
Ex<-mean(sin(x))*pi/3;
print(c(Ex,1-cos(pi/3)));
```
And you can see that the results are pretty close.

## Question 5.7
Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $\theta$ by the
antithetic variate approach and by the simple Monte Carlo method. Compute
an empirical estimate of the percent reduction in variance using the antithetic
variate. Compare the result with the theoretical value from Exercise 5.6.

## answer
According to 5.6,$\theta=e-1=1.71828...$.First, the variance of the two methods is calculated and the expectation definition is used to obtain the following results:
$E(e^{-2u})=\frac{e^2-1}{2e^2},E(e^{2u})=\frac{e-1}{e},E(e^{u})=e-1,$
$E(e^{2u})=\frac{e^2-1}{2},$
$Cov(e^u,e^{1-u})=e-E(e^u)E(e^{1-u})=e-{(e-1)}^2,$
$Var(e^u)=Ee^{2u}-{E(e^u)}^2,$
$Var(e^{1-u})=\frac{(3-e)(e-1)}{2}.$

then $Var_{mc}=Var(e^u)=E(e^{2u})-{E(e^{u})}^2=2e-\frac{e^2}{2}-\frac{3}{2}$

$Var_{av}=var(\frac{e^u+e^{1-u}}{2})$
$=[Var(e^u)+Var(e^{1-u})+2Cov(e^u,e^{1-u})]/4$
$=\frac{10e-3e^2-5}{4}$

so $\frac{Var_{mc}-Var_{av}}{Var_{mc}}*100\%=98.384\%$

Namely, the theoretical value of the relative variance percentage is $98.384\%$.

The next is  using two separate approaches (first, the antithetic variate approach, and second, the simple Monte Carlo method)$\theta$ to estimate and calculate their variances and percent reduction in variance,
And the mean of the first method(the estimation of $\theta$),the mean of the second method(the estimation of $\theta$),the variance of the first method,the variance of the second method and the percentage of the relative variance is:

```{r}
n<-1e5
ux<-runif(n/2,min=0,max=1);
uy<-runif(n,min=0,max=1);
ex<-(exp(ux)+exp(1-ux))/2;# antithetic variate approach-s1
ey<-exp(uy);#  simple Monte Carlo method-s2
meex<-mean(ex);# the mean of s1
meey<-mean(ey);# the mean of s2
vaex<-var(ex);# the variance of s1
vaey<-var(ey);# the variance of s2
pr<-(vaey-vaex)/vaey; # empirical estimate of the percent reduction in variance
print(c(meex,meey,vaex,vaey,pr));
```

As was shown in several experiments, the first approach
(antithetic variate approach) is better,Both methods estimate $\theta$ well, and the percentage of relative variance is very close to the theoretical value $98.384\%$.


## Question 5.11
If $\hat{\theta}_1$ and $\hat{\theta}_2$ are unbiased estimators of $\theta$, and $\hat{\theta}_1$ and $\hat{\theta}_2$are antithetic, we derived that $c^∗=\frac{1}{2}$is the optimal constant that minimizes the variance of
$\hat{\theta}_c= c\hat{\theta}_1+(1 − c)\hat{\theta}_2$ . Derive$c^∗$for the general case. That is, if $\hat{\theta}_1$ and $\hat{\theta}_2$ are any two unbiased estimators of $\theta$, find the value $c^∗$ that minimizes the variance of the estimator$\hat{\theta}_c$ = c$\hat{\theta}_1+(1 − c)\hat{\theta}_2$ in equation (5.11). ($c^∗$ will be a function of the variances and the covariance of the estimators.)

## answer
According to Equation (5.11) :
$var(c\hat{\theta}_1+(1 − c)\hat{\theta}_2)=var(\hat{\theta}_2)+c^2var(\hat{\theta}_1-\hat{\theta}_2)+2cCov(\hat{\theta}_2,\hat{\theta}_1-\hat{\theta}_2)$,to find its minimum,when $c^*=\frac{Cov(\hat{\theta}_2,\hat{\theta}_1-\hat{\theta}_2)}{var(\hat{\theta}_1-\hat{\theta}_2)}$,$var(c\hat{\theta}_1+(1 − c)\hat{\theta}_2)$ gets its minimum value.


## Question 5.13

Find two importance functions f1 and f2 that are supported on $(0,\infty)$ and are ‘close’ to
$g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2},x>1.$Which of your two importance functions should produce the smaller variance in estimating $\int_1^{\infty}\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}dx$ by importance sampling? Explain.

## answer

Take the probability density function over $(1,\infty)$,the probability density functions are $f_1(x)=e^{1-x}$ and $f_2(x)=\frac{7}{t^8}$,the distribution functions are, in turn,$F_1=1-e^{1-t}$,$F_2=1-\frac{1}{x^7}$,the inverse functions are, in turn,$F_1^{-1}=1-ln(1-y),F_2^{-1}=\frac{1}{(1-y)^{\frac{1}{7}}}$

Figure is as follows:

```{r}
x<-seq(1,20,by=0.2);
y1<-x^2*exp(-x^2/2)/sqrt(2*pi);
y2<-exp(1-x);
y3<-7/x^8;
plot(x,y1,type="l",col = "black",xlim=c(0,22),ylim = c(0, 1.2));
lines(x,y2,type="l",col = "blue");
lines(x,y3,type="l",col = "red")
legend(12,1,c("The function","f1","f2"),col=c("black","blue","red"),text.col=c("black","blue","red"),lty=c(1,1,1))
```

Use random number generation and inverse transformation method to calculate:

```{r}
n=10000
t<-runif(n,0,1)
x1<-1-log(1-t)
x2<-1/(1-t)^(1/7)
y11<-(x1)^2*exp(-(x1)^2/2)/sqrt(2*pi)
y12<-(x2)^2*exp(-(x2)^2/2)/sqrt(2*pi)
y2<-exp(1-(x1))
y3<-7/(x2)^8
m1<-mean(y11/y2)
m2<-mean(y12/y3)
sd1<-sd(y11/y2)
sd2<-sd(y12/y2)
s=0.400626#True integral value
print(c(s,m1,m2,sd1,sd2))
```
The first value is the true value, the second and third are the simulated values corresponding to $f_1$ and $f_2$, and the fourth and fifth are the corresponding standard deviations.It can be seen from the results that the estimate of $f_1$is good and has the lowest variance, because the $f_1$ function is more compatible with the integrand, while the $f_2$ is significantly worse.

## Question 5.15

Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

## answer


The probability density function over each cell is $\frac{e^{-x}}{e^{-\frac{j-1}{5}}-e^{-\frac{j}{5}}},\frac{j-1}{5}<x<\frac{j}{5}$,the inverse transformation method is applied to each interval.
The code and figure are shown below:

```{r}
n=10000#The number of random numbers
k=5#Number of intervals
r=n/k#Number of iterations per cell
N=50#Number of iterations
T2<- numeric(k)
T3<-numeric(k)
estimates <- matrix(0, N, 1)

for (i in 1:N) {
  for (j in 1:k){
    t<-runif(r,0,1)
    x<--log(exp(-(j-1)/5)-(exp((1-j)/5)-exp(-j/5))*t)
    T2[j]<-mean((exp((1-j)/5)-exp((-j)/5))/(1+x^2))
  }
  estimates[i, 1] <- sum(T2)
}
ss<-mean(estimates)
sd1=sd(estimates)
s=0.5247971#True integral value
print(c(ss,s,sd1))
```
The values above are respectively the mean of Stratified Importance Sampling,the true value,corresponding variance of Stratified Importance Sampling.According to the mean and variance, it is obvious that the result is better than Example 5.10.

## Question 6.4

Suppose that X1, . . . , Xn are a random sample from a from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for the parameter $\mu$. Use a Monte Carlo method to obtain an empirical estimate of the confidence level.

## answer

According to the question,$y_i=ln(x_i)\sim N(\mu,{\sigma}^2)$,let
$S^2=\frac{{\sum_{i=1}^n (y_i-\bar{y})}^2}{n-1},\bar{y}=\frac{\sum_{i=1}^ny_i}{n}$,it is known from mathematical statistics knowledge that:$\frac{\sqrt{n}(\bar{y}-\mu)}{S}\sim t_{n-1}$,According to this property, the  confidence interval of the parameter $\mu$is constructed:
$(\bar{y}-S\frac{t_{0.025}(n-1)}{\sqrt{n}},\bar{y}+S\frac{t_{0.025}(n-1)}{\sqrt{n}})$,where $t_{0.025}(n-1)$is the 0.025 quantile 
of t(n-1) distribution.

let $\mu=1,\sigma=1,n=9$,Then, the confidence interval is $(\bar{y}-S\frac{t_{0.025}(n-1)}{\sqrt{n}},\bar{y}+S\frac{t_{0.025}(n-1)}{\sqrt{n}})$=$(\bar{y}-0.7687S,\bar{y}+0.7687S)$.Rlnorm is used to generate logarithmic normal distribution random numbers, and the number of simulation m=100000. The following is the program simulation:

```{r}
m=1e5
n=0
k=9
u=1
v=1
for (i in c(1:m)){
  x=rlnorm(k,u,v)
  y=log(x)
  mm=mean(y)
  ss=sd(y)
  l1=mm-0.7687*ss
  l2=mm+0.7687*ss
  if (l1<u && u<l2)
  {
    n=n+1
  }
}
ECP=n/m
print(c('The empirical confidence level ECP is:',ECP))
print(c('The actual confidence level CL is:',0.95))
```
It can be seen that the empirical confidence level is very close to the actual confidence level.


## Question 6.5

Suppose a 95% symmetric $t-$interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the $t-$interval for random samples of $χ^2(2)$data with sample size n = 20. Compare your $t-$interval results with the
simulation results in Example 6.4. (The $t-$interval should be more robust to departures from normality than the interval for variance.)

## answer

---
The mean of $χ^2(2)$ is 2,let $\mu=2,\sigma=1,n=20$,and $S^2=\frac{{\sum_{i=1}^n (x_i-\bar{x})}^2}{n-1},\bar{x}=\frac{\sum_{i=1}^nx_i}{n}$.Then, the confidence interval is $(\bar{x}-S\frac{t_{0.025}(n-1)}{\sqrt{n}},\bar{x}+S\frac{t_{0.025}(n-1)}{\sqrt{n}})$=$(\bar{x}-0.468S,\bar{x}+0.468S)$.Rchisq was used to generate chi-square distribution random numbers, and the simulation times were m=100000. The following is the program simulation:

```{r}
m=1e5
n=20
k=0
u=2
v=2
for (i in c(1:m)){
  x=rchisq(n,u)
  mm=mean(x)
  ss=sd(x)
  l1=mm-0.468*ss
  l2=mm+0.468*ss
  if (l1<u && u<l2)
  {
    k=k+1
  }
}
ECP=k/m
print(c('The empirical confidence level ECP is:',ECP))
print(c('The actual confidence level CL is:',0.95))
```
It can be seen that there is not much difference between the empirical confidence level and the actual confidence level.


## Question 6.7

Estimate the power of the skewness test of normality against symmetric
Beta$(\alpha,\alpha)$distributions and comment on the results. Are the results different
for heavy-tailed symmetric alternatives such as $t(ν)$?


## answer

Let's take the significance level  $\alpha=0.1,$ the distribution parameter is $b=(0.5,1,1.5,2,2.5,3,3.5,4,4.5,5)$
then the test procedure for the skewness of the symmetric $\beta$distribution is as follows:

```{r}
sk<-function(x){
  xbar<-mean(x)
  m3<-mean((x-xbar)^3)
  m2<-mean((x-xbar)^2)
  return(m3/m2^1.5)
}

alpha<-0.1
b<-c(0.5,1,1.5,2,2.5,3,3.5,4,4.5,5)
n<-40#Sample size
m<-2500#Number of repetitions
cv<-qnorm(1-alpha/2,0,sqrt(6*(n-2)/((n+1)*(n+3))))
sktests<-numeric(m)
pwr<-numeric(length(b))
for (j in 1:length(b)){
for (i in 1:m){
  x<-rbeta(n,b[j],b[j])
  sktests[i]<-as.integer(abs(sk(x)>=cv))
  }
pwr[j]<-mean(sktests)
}
plot(b,pwr,type="b",xlab=bquote(b),ylim=c(0,0.05))
abline(h=.1,lty=3)
se<-sqrt(pwr*(1-pwr)/m)
lines(b,pwr+se,lty=3)
lines(b,pwr-se,lty=3)
```


The above result is the skewness power of the symmetric $\beta(b,b)$distribution under different parameters


Consider the t-distribution as follows, and consider the parameter range of t-distribution (1:100):

```{r}
sk<-function(x){
  xbar<-mean(x)
  m3<-mean((x-xbar)^3)
  m2<-mean((x-xbar)^2)
  return(m3/m2^1.5)
}

alpha<-0.1
b<-c(seq(1,100,by=1))
n<-40#Sample size
m<-2500#Number of repetitions
cv<-qnorm(1-alpha/2,0,sqrt(6*(n-2)/((n+1)*(n+3))))
sktests<-numeric(m)
pwr<-numeric(length(b))
for (j in 1:length(b)){
   for (i in 1:m){
    x<-rt(n,df=b[j])
    sktests[i]<-as.integer(abs(sk(x)>=cv))
   }
   pwr[j]<-mean(sktests)
}
plot(b,pwr,type="b",xlab=bquote(b),ylim=c(0,0.6))
abline(h=.1,lty=3)
se<-sqrt(pwr*(1-pwr)/m)
lines(b,pwr+se,lty=3)
lines(b,pwr-se,lty=3)
```

It can be seen that as the degree of freedom increases, the power of skewness decreases.


## Question 6.8

Refer to Example 6.16. Repeat the simulation, but also compute the F test of equal variance,at significance level $\hat{\alpha}\dot{=}0.055$.Compare the power of the Count Five test and F test for small, medium, and large sample sizes. (Recall that the F test is not applicable for non-normal distributions.)


## answer

The sample size is n=10,200,5000 represent small sample, medium sample, and large sample,Significant level is $\alpha=0.055$,$\sigma_1^2=\sigma_2^2=4,\mu_1=\mu_2=0$


```{r}
count5test<-function(x,y){
  X<-x-mean(x)
  Y<-y-mean(y)
  outx<-sum(X>max(Y))+sum(X<min(Y))
  outy<-sum(Y>max(X))+sum(Y<min(X))
  return(as.integer(max(c(outx,outy))>5))
}

a<-0.055
sigma1<-1
sigma2<-1.5
n<-c(10,100,5000)
k<-length(n)
m<-3000
power<-array(2*k, c(k, 2))
for (i in 1:k){
  power[i,1]<-mean(replicate(m,expr={
    x1<-rnorm(n[i],0,sigma1)
    y1<-rnorm(n[i],0,sigma2)
    count5test(x1,y1)
  }))
  
  power[i,2]<-mean(replicate(m,expr={
    x2<-rnorm(n[i],0,sigma1)
    y2<-rnorm(n[i],0,sigma2)
    s<-var(x2)/var(y2)
    l1<-qf(1-a/2,n[i]-1,n[i]-1)
    l2<-1/l1
    as.numeric((s<l2)|(s>l1))
  }))
  
}

print(power)
```

Each line above is respectively the power of Count Five and F test under the conditions of small sample, medium sample and large sample. When the sample size is large, the power of both is close to 1.


## Question 6.C

Repeat Examples 6.8 and 6.10 for Mardia’s multivariate skewness test. Mardia [187] proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If $X$and $Y$ are iid, the multivariate population skewness $\beta_{1,d}$ is defined by Mardia as 

$$\beta_{1,d}=E[(X-\mu)^{T}\Sigma^{-1}(Y-\mu)]^3.$$

Under normality, $\beta_{1,d}$= 0. The multivariate skewness statistic is 
$$b_{1,d}=\frac{1}{n^2}\sum_{i,j=1}^n((X_i-\bar{X})^T\hat{\Sigma}^{-1}(X_j-\bar{X}))^3,$$


where $\hat{\Sigma}$is the maximum likelihood estimator of covariance. Large values of $b_{1,d}$ are significant. The asymptotic distribution of nb1,d/6 is chisquared with
$d(d + 1)(d + 2)/6$ degrees of freedom.


## answer

Example 6.8:

For example 6.8, take $\alpha=0.05$, take different sample sizes, and repeat the sample 1000 times. The following results can be obtained:


```{r}
rmvn.eigen <- function(n, mu, Sigma) {
  # generate random vectors from MVN(mu, Sigma)
  # dimension is inferred from mu and Sigma
  d <- length(mu)
  ev <- eigen(Sigma, symmetric = TRUE)
  lambda <- ev$values
  V <- ev$vectors
  C <- V %*% diag(sqrt(lambda)) %*% t(V)
  Z <- matrix(rnorm(n*d), nrow = n, ncol = d)
  X <- Z %*% C + matrix(mu, n, d, byrow = TRUE)
  X
}

alpha<-0.05
cv<-qchisq(1-alpha,df=4)
m<-1000#Number of repetitions for different sample sizes
A<-array(c(5,3,3,7),dim=c(2,2))
n <- c(10, 20, 30, 50, 100, 500)
mu<-array(c(0,0),dim=c(1,2))
sv<-numeric(length(n))
for (i in 1:length(n)){
    cc<-numeric(m)
    for (k in 1:m){
      zX<-numeric(2*n[i])
      dim(zX)<-c(n[i],2)
      X<-numeric(n[i]*2)
      dim(X)<-c(n[i],2)
      X<-rmvn.eigen(n[i],mu,A)
      zX[,1]<-X[,1]-mean(X[,1])
      zX[,2]<-X[,2]-mean(X[,2])
      la<-sum(zX[,1]*zX[,1])/(n[i]-1)
      lb<-sum(zX[,2]*zX[,2])/(n[i]-1)
      lc<-sum(zX[,1]*zX[,2])/(n[i]-1)
      L<-solve(array(c(la,lc,lc,lb),dim=c(2,2)))
      dim(zX)<-c(n[i],2)
      s<-zX%*%L%*%t(zX)
      count<-sum(s^3)/n[i]/6
      cc[k]<-as.integer(count>cv)
    }
  sv[i]<-mean(cc)
} 
    
for (d in 1:length(n)) {
  
  print(paste0("when n=",n[d],",the empirical estimate for the first type of error rate is:" ,sv[d]))
  
}
```

It can be seen that when the sample size is large, the empirical estimate is close to $\alpha$.




Example 6.10:

The following example is 6.10. Sample size n=30,$\alpha=0.1$, and covariance matrix is respectively$$
 \left[
 \begin{matrix}
   5 & 3 \\
   3 & 7  \\
  \end{matrix}
  \right] 
$$
and 
$$
 \left[
 \begin{matrix}
   6 & 1  \\
   1 & 4 \\
  \end{matrix}
  \right]
$$


The code is as follows:

```{r}
rmvn.eigen <- function(n, mu, Sigma) {
  # generate random vectors from MVN(mu, Sigma)
  # dimension is inferred from mu and Sigma
  d <- length(mu)
  ev <- eigen(Sigma, symmetric = TRUE)
  lambda <- ev$values
  V <- ev$vectors
  C <- V %*% diag(sqrt(lambda)) %*% t(V)
  Z <- matrix(rnorm(n*d), nrow = n, ncol = d)
  X <- Z %*% C + matrix(mu, n, d, byrow = TRUE)
  X
}

alpha<-0.1
cv<-qchisq(1-alpha,df=4)
m<-1000#Number of duplications of different Epsilon value samples
B<-numeric(8)
dim(B)<-c(2,2,2)
B[,,1]<-array(c(5,3,3,7),dim=c(2,2))
B[,,2]<-array(c(60,1,1,10),dim=c(2,2))
n <-30
epsilon<-c(seq(0,1,0.05))
mu<-array(c(0,0),dim=c(1,2))
sv<-numeric(length(epsilon))
for (i in 1:length(epsilon)){
    e<-epsilon[i]
    cc<-numeric(m)
    for (k in 1:m){
      zX<-numeric(2*n)
      dim(zX)<-c(n,2)
      X<-numeric(n*2)
      dim(X)<-c(n,2)
      a<-sample(c(1,2),replace=TRUE,size=n,prob=c(e,1-e))
      for (j in 1:n){
       A<-B[,,a[j]]
       X[j,]<-rmvn.eigen(1,mu,A)
      }
       zX[,1]<-X[,1]-mean(X[,1])
       zX[,2]<-X[,2]-mean(X[,2])
       la<-sum(zX[,1]*zX[,1])/(n-1)
       lb<-sum(zX[,2]*zX[,2])/(n-1)
       lc<-sum(zX[,1]*zX[,2])/(n-1)
       L<-solve(array(c(la,lc,lc,lb),dim=c(2,2)))
       dim(zX)<-c(n,2)
       s<-zX%*%L%*%t(zX)
       count<-sum(s^3)/n/6
       cc[k]<-as.integer(count>cv)
    }
  sv[i]<-mean(cc)
} 

plot(epsilon,sv,type="b",xlab=bquote(epsilon),ylim=c(0,1))
abline(h=0.1,lty=3)
```

When $epsilon$is equal to 0 or 1, the overall distribution is two-dimensional normal.




## Discussion
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one
method and 0.676 for another method. Can we say the powers are different at 0.05 level?

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)What is the corresponding hypothesis test problem?

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test?

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3)What information is needed to test your hypothesis?
  
## answer
  
  Suppose the theoretical power of the first method is $\mu_1$, and that of the second method is $\mu_2$

(1)he corresponding hypothesis questions are:$H_0:\mu_1=\mu_2,H_0:\mu_1\not=\mu_2$

(2)Let the empirical power of the first and second methods is respectively $X_1,X_2$.Then $X_1\sim B(n,\mu_1),X_2\sim B(n,\mu_2)$, When the sample size is sufficiently large, according to the central limit theorem, the corresponding centralization variables tend to be standard normal distribution. We can select $McNemar$ test, etc.


(3)Multiple simulations were conducted to record the empirical power of the two methods each time, and the hypothesis was tested using the large sample method.


## Question 7.1

Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2.

## answer

The program is as follows:

```{r}
library(bootstrap)
n<-dim(law)[1]
dd<-cor(law[,1],law[,2])
d<-numeric(n)
for (i in 1:n){
  A<-law[(1:n)[-i],]
  d[i]<-cor(A[,1],A[,2])
}
bias<-(n-1)*(mean(d)-dd)
sD<-sqrt((n-1)*mean((d-mean(d))^2))
print(paste0("The bias of the correlation coefficient is:",bias,",The standard deviation of the correlation coefficient is:",sD))
```


## Question 7.5

Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the
mean time between failures $1/\lambda$ by the standard normal, basic, percentile,
and BCa methods. Compare the intervals and explain why they may differ.


## answer

```{r}
library(boot)
#bootstrap
rn<-2000
P<-aircondit$hours
dim(P)<-c(dim(aircondit)[1],1)
n<-nrow(P)
r<-numeric(n) 
for (i in 1:n){
  a<-sample(1:n,size=n,replace=TRUE)
  A<-P[a]
  r[i]<-mean(A)
}

#compute the confidence interval
stat <- function(dat, index) {
  dat[index]}

boot.obj <-boot(r, statistic = stat, R=rn)
boot.ci(boot.obj, conf=0.95,type=c("norm","basic","perc", "bca"))
```

The 95% confidence interval results of $1/\lambda$ using four different methods are shown in the figure above.

It can be seen that the results are slightly different, mainly because the optimal sample size requirements of the four methods are different. For example, norm requires a large sample property, while percentile uses the sample empirical distribution generated by bootstrap to estimate the confidence interval.


## Question 7.8

Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard
error of $\bar{\theta}$.


## answer

R code for estimating the deviation and Standard deviation of $\theta$using the Jackknife method:

```{r}
data(scor, package = "bootstrap")
s<-cov(scor)
l<-eigen(s)$values
l1<-max(l)
theta.hat<-l1/sum(l)
n <- nrow(scor)
print (theta.hat)
#jackknife calculate the bias and standard deviation
theta.jack <- numeric(n)
for (i in 1:n){
   JA<-cov(scor[-i,])
   g<-eigen(JA)$values
   g1<-max(g)
   theta.jack[i]<-g1/sum(g)}
bias <- (n - 1) * (mean(theta.jack) - theta.hat)
se<-sqrt((n-1)*mean((theta.jack-mean(theta.jack))^2))
print(paste0("The bias is:",bias,",the standard deviation is:",se)) 
```
The above are the bias and standard deviation results of $\theta$estimated by the Jackknife method.


## Question 7.11

In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.

## answer

Carry out 53*(53-1) cycle fitting, remove two different rows of data each time, calculate the error, and finally calculate the average. The program is as follows:

```{r}
library(DAAG)
l<-length(ironslag$chemical)
D<-numeric(l*2)
dim(D)<-c(l,2)
X<-ironslag$chemical
Y<-ironslag$magnetic
e1<-e2<-e3<-e4<-numeric(l*(l-1))
n<-1
for (i in 1:l){
  k<-(1:l)[-i]
  for (j in k){
    index<-c(i,j)
    x<-X[-index]
    y<-Y[-index]
    
    L1<-lm(y~x)
    yhat1 <- L1$coef[1] + L1$coef[2] * X[index]
    e1[n] <- sum(Y[index] - yhat1)^2/2
    
    L2 <- lm(y ~ x + I(x^2))
    yhat2 <- L2$coef[1] + L2$coef[2] * X[index] +
      L2$coef[3] * X[index]^2
    e2[n] <- sum(Y[index] - yhat2)^2/2
    
    L3 <- lm(log(y) ~ x)
    logyhat3 <- L3$coef[1] + L3$coef[2] * X[index]
    yhat3 <- exp(logyhat3)
    e3[n] <- sum(Y[index] - yhat3)^2/2
    
    L4 <- lm(log(y) ~ log(x))
    logyhat4 <- L4$coef[1] + L4$coef[2] * log(X[index])
    yhat4 <- exp(logyhat4)
    e4[n] <- sum(Y[index] - yhat4)^2/2
    n<-n+1
  }
}
 
print(c(mean(e1),mean(e2),mean(e3),mean(e4))) 
```

According to the results, the second fitting model may be most suitable for the data.


## Question 8.3
&ensp;The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.

## Answer

Combined with Permutation method and Count5test method, program under different sample sizes. The program is as follows:

```{r}
count5test<-function(x,y){
  X<-x-mean(x)
  Y<-y-mean(y)
  outx<-sum(X>max(Y))+sum(X<min(Y))
  outy<-sum(Y>max(X))+sum(Y<min(X))
  return(as.integer(max(c(outx,outy))>5))
}

n1<-5
n2<-15
mu1<-mu2<-0
sigma1<-sigma2<-1
m<-1000#Number of repetitions
x<-rnorm(n1,mu1,sigma1)
y<-rnorm(n2,mu2,sigma2)
z<-c(x,y)
K<-1:length(z)
D<-numeric(m)
for (i in (1:m) ){
  k<-sample(K,size=(n1+n2)/2,replace=FALSE)
  x1<-z[k]
  y1<-z[-k]
  D[i]<-count5test(x1,y1)
}
p<-mean(D)
print(p)
```
The above is the probability of the first type of error under different sample numbers (n1=5,n2=15), which can be seen as relatively small.



## Question 
Design experiments for evaluating the performance of the NN,energy, and ball methods in various situations.

&emsp;&emsp;Unequal variances and equal expectations

&emsp;&emsp;Unequal variances and unequal expectations

&emsp;&emsp;Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions)

&emsp;&emsp;Unbalanced samples (say, 1 case versus 10 controls)

&emsp;&emsp;Note: The parameters should be chosen such that the powers are distinguishable (say, range from 0.3 to 0.8).


## Answer

Use the power calculation program given in PPT,the following can be obtained:

(1)In the multi-dimensional case (considering two-dimensional normal distribution), with the same expectation, different covariance matrices:

```{r}
library(MASS)
library(boot)
library(RANN)
library(energy)
library(Ball)
Tn <- function(z, ix, sizes,k) {
n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
if(is.vector(z)) z <- data.frame(z,0);
z <- z[ix, ];
NN <- nn2(data=z, k=k+1) # what's the first column?
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
(i1 + i2) / (k * n)
}
A<-matrix(data=c(1,0,0,1),nrow=2)
a<-c(1,1)
B<-matrix(data=c(2,-1,-1,2),nrow=2)
b<-c(1,1)
m <- 1e3; 
k<-3;
p<-2;
mu <- 0.5;
set.seed(12345)
n1 <- n2 <- 50; 
R<-999; 
n <- n1+n2; 
N = c(n1,n2)
eqdist.nn <- function(z,sizes,k){
  boot.obj <- boot(data=z,statistic=Tn,R=R,
                   sim = "permutation", sizes = sizes,k=k)
  ts <- c(boot.obj$t0,boot.obj$t)
  p.value <- mean(ts>=ts[1])
  list(statistic=ts[1],p.value=p.value)
}
p.values <- matrix(NA,m,3)
for(i in 1:m){
  x <- mvrnorm(n1,a,A);
  y <- mvrnorm(n2,b,B);
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,R=R,seed=i*12345)$p.value
}

alpha <- 0.1;
pow <- colMeans(p.values<alpha)
print(pow)
```
Above are the corresponding power of NN, Energy and ball methods in the case of multi-dimensional (considering two-dimensional normal distribution) with the same expectation and different covariance matrices. The greater the power, the better the effect. The result shows that the third method is better.

(2)In the multi-dimensional case (considering two-dimensional normal distribution), with different expectations and different covariance matrices:

```{r}
library(MASS)
library(boot)
library(RANN)
library(energy)
library(Ball)
Tn <- function(z, ix, sizes,k) {
n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
if(is.vector(z)) z <- data.frame(z,0);
z <- z[ix, ];
NN <- nn2(data=z, k=k+1) # what's the first column?
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
(i1 + i2) / (k * n)
}
A<-matrix(data=c(1,0,0,1),nrow=2)
a<-c(1,1)
B<-matrix(data=c(1.3,0.5,0.5,0.7),nrow=2)
b<-c(0.1,0.5)
m <- 1e3;
k<-3;
p<-2;
set.seed(12345)
n1 <- n2 <- 50; 
R<-999; 
n <- n1+n2; 
N = c(n1,n2)
eqdist.nn <- function(z,sizes,k){
  boot.obj <- boot(data=z,statistic=Tn,R=R,
                   sim = "permutation", sizes = sizes,k=k)
  ts <- c(boot.obj$t0,boot.obj$t)
  p.value <- mean(ts>=ts[1])
  list(statistic=ts[1],p.value=p.value)
}
p.values <- matrix(NA,m,3)
for(i in 1:m){
  x <- mvrnorm(n1,a,A);
  y <- mvrnorm(n2,b,B);
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,R=R,seed=i*12345)$p.value
}

alpha <- 0.1;
pow <- colMeans(p.values<alpha)
print(pow)
```
Above are the corresponding power of NN, Energy and ball methods in the case of multi-dimensional (considering two-dimensional normal distribution) with different expectation and different covariance matrices. The greater the power, the better the effect. The latter two are better.

(3)In the non-normal case:
T distribution of X, bimdel distribution of Y (mixing of two normal distributions, mixing ratio q):

```{r}
library(boot)
library(RANN)
library(energy)
library(Ball)
Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1) # what's the first column?
  block1 <- NN$nn.idx[1:n1,-1]
  block2 <- NN$nn.idx[(n1+1):n,-1]
  i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
  (i1 + i2) / (k * n)
}

m <- 1e3; 
k<-3; p<-2; 
mu <- 0.5; set.seed(12345)
n1 <- n2 <- 50; 
R<-999; 
n <- n1+n2; 
N = c(n1,n2)
q<-0.7#Proportional parameter of mixed distribution
eqdist.nn <- function(z,sizes,k){
  boot.obj <- boot(data=z,statistic=Tn,R=R,
                   sim = "permutation", sizes = sizes,k=k)
  ts <- c(boot.obj$t0,boot.obj$t)
  p.value <- mean(ts>=ts[1])
  list(statistic=ts[1],p.value=p.value)
}
p.values <- matrix(NA,m,3)
for(i in 1:m){
  x <- matrix(rt(n1*p,df=1),ncol=p);
  y<-numeric(n1*p)
  u<-runif(n1*p)
  l<-which(u<=q)
  y[l]<-rnorm(length(l));
  y[-l]<-rnorm(n1*p-length(l),mean=mu);
  y<-matrix(y,ncol=p)
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,R=R,seed=i*12345)$p.value
}
alpha <- 0.1;
pow <- colMeans(p.values<alpha)
print(pow)
```
Above are the corresponding power of NN,energy and ball methods in t distribution of X and BIModel distribution of Y (the mixture of two normal distributions with mixing ratio of q). The greater the power, the better the effect. It can be seen from the results that the latter two methods have very good effects.


(4)In the case of different sample numbers, taking into account the definition of power, the distribution parameters of X and Y are different:

```{r}
library(MASS)
library(boot)
library(RANN)
library(energy)
library(Ball)
Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z,0);
  z <- z[ix, ];
  NN <- nn2(data=z, k=k+1) # what's the first column?
  block1 <- NN$nn.idx[1:n1,-1]
  block2 <- NN$nn.idx[(n1+1):n,-1]
  i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
  (i1 + i2) / (k * n)
}
A<-matrix(data=c(1,0,0,1),nrow=2)
a<-c(1,1)
B<-matrix(data=c(1,0,0,3),nrow=2)
b<-c(1,1)
m <- 1e3; 
k<-3;
p<-2;
mu <- 0.5;
set.seed(12345)
n1 <-20;
n2<-200;
R<-999;
n <- n1+n2; 
N = c(n1,n2)
eqdist.nn <- function(z,sizes,k){
  boot.obj <- boot(data=z,statistic=Tn,R=R,
                   sim = "permutation", sizes = sizes,k=k)
  ts <- c(boot.obj$t0,boot.obj$t)
  p.value <- mean(ts>=ts[1])
  list(statistic=ts[1],p.value=p.value)
}
p.values <- matrix(NA,m,3)
for(i in 1:m){
  x <- mvrnorm(n1,a,A);
  y <- mvrnorm(n2,b,B);
  z <- rbind(x,y)
  p.values[i,1] <- eqdist.nn(z,N,k)$p.value
  p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
  p.values[i,3] <- bd.test(x=x,y=y,R=R,seed=i*12345)$p.value
}

alpha <- 0.1;
pow <- colMeans(p.values<alpha)
print(pow)
```
Above are the corresponding power of NN, Energy and ball methods under different sample sizes. So you can see in this case the third way is a little bit better


## Question 9.4

Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see Exercise 3.2). For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distribution. Also, compute the acceptance rates of each chain.

## answer

$f(t)=\frac{1}{2}e^{-|x|}$,according to the question,$r(x_t,y)=\frac{f(Y)}{f(X_t)}=e^{|x_t|-|y|}$

```{r}
LS<-function(x){
  f<-exp(-abs(x))/2
  return(f)
}

rw.Metropolis<-function(sigma,x0,N){
  x<-numeric(N)
  x[1]<-x0
  u<-runif(N)
  k<-0
  for (i in 2:N){
   y<-rnorm(1,x[i-1],sigma)
   if (u[i] <= LS(y) / LS(x[i-1]))
  {x[i] <- y}
else
 {x[i] <- x[i-1]
  k<-k+1
 }
  }
  return(list(x=x,k=k))
}


N<-5000
sigma<-c(0.1,0.5,1,5,20,50)
x0<-10
rw1<-rw.Metropolis(sigma[1],x0,N)
rw2<-rw.Metropolis(sigma[2],x0,N)
rw3<-rw.Metropolis(sigma[3],x0,N)
rw4<-rw.Metropolis(sigma[4],x0,N)
rw5<-rw.Metropolis(sigma[5],x0,N)
rw6<-rw.Metropolis(sigma[6],x0,N)

lk<-(N-c(rw1$k,rw2$k,rw3$k,rw4$k,rw5$k,rw6$k))/N#Acceptance rate
for (l in 1:length(lk)){
  print(paste('The',l,'th chain s corresponding normal standard deviation is:',sigma[l],',Accept rate is :',lk[l]))
}

#par(mfrow = c(3, 2))

plot(rw1$x,type='l',xlab=expression(paste(sigma,'=0.1')))
plot(rw2$x,type='l',xlab=expression(paste(sigma,'=0.5'))) 
plot(rw3$x,type='l',xlab=expression(paste(sigma,'=1')))
plot(rw4$x,type='l',xlab=expression(paste(sigma,'=5')))
plot(rw5$x,type='l',xlab=expression(paste(sigma,'=20')))
plot(rw6$x,type='l',xlab=expression(paste(sigma,'=50')))

mtext(expression(paste("The performance of different ",sigma)), side =3, line = -2.5, outer = TRUE)
```


It can be seen that the second to fourth $\sigma$ values produce better results



## Question 9.4-Gelman-Rubin method
## answer

Only consider the case where $\sigma=0.5$ and the initial value of is different, the code is as follows (15000 times):

```{r}
set.seed(3426)
Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi) #row means
  B <- n * var(psi.means) #between variance est.
  psi.w <- apply(psi, 1, "var") #within variances
  W <- mean(psi.w) #within est.
  v.hat <- W*(n-1)/n + (B/n) #upper variance est.
  r.hat <- v.hat / W #G-R statistic
  return(r.hat)
}

LS<-function(x){
  f<-exp(-abs(x))/2
  return(f)
}

rw.Metropolis<-function(sigma,x0,N){
  x<-numeric(N)
  x[1]<-x0
  u<-runif(N)
  k<-0
  for (i in 2:N){
    y<-rnorm(1,x[i-1],sigma)
    if (u[i] <= LS(y) / LS(x[i-1]))
    {x[i] <- y}
    else
    {x[i] <- x[i-1]
    }
  }
  return(x)
}


RR<-1.2
sigma<-0.5#Consider only 0.5 for a different initial value
Rr<-2
x0 <- c(-10, -5, 5, 10)#The initial value
N<-15000#Let's do it 15000 times, and then let's do the index
rw<-matrix(rep(0,length(x0)*N),nrow=length(x0))

for (rr in 1:length(x0)){
    sig<-x0[rr]
    rw[rr,]<-rw.Metropolis(sigma,sig,N)
}

psi <- t(apply(rw, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))


#plot psi for the six chains
b<-1000
par(mfrow=c(1,1)) #restore default
#plot the sequence of R-hat statistics
rhat <- rep(0, N)
for (j in (b+1):N)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):N], type="l", xlab="", ylab="R")
abline(h=1.1, lty=2)
lines(c(0,N),c(1.2,1.2),type='b')

n<-5500#Look at the graph, let's start at 5500
IN<-numeric(N)
while (Rr>=RR){
  n<-n+1
  pp<-psi[,1:n]
  IN[n]<-Gelman.Rubin(pp)
  Rr<-IN[n]
}

print(paste('When we get to the ',n,',the index we want is less than 1.2'))

print('Here is the mean of each chain:')

par(mfrow=c(1,1))
plot(psi[1,],type='l',col=1,lwd=1.3,xlab="",ylab="",ylim=c((min(psi)-2),(max(psi)+2)))
par(new=TRUE)
plot(psi[2,],type='l',col=2,lwd=1.3,xlab="",ylab="",ylim=c((min(psi)-2),(max(psi)+2)))
par(new=TRUE)
plot(psi[3,],type='l',col=3,lwd=1.3,xlab="",ylab="",ylim=c((min(psi)-2),(max(psi)+2)))
par(new=TRUE)
plot(psi[4,],type='l',col=4,lwd=1.3,xlab="",ylab="",ylim=c((min(psi)-2),(max(psi)+2)))

legend('topright',legend = c('x0=-10','x0=-5', 'x0=5','x0=10'), col = c(1,2,3,4),lty=c(1,2,3,4))
mtext("Mean of each chain", side =3, line = -2.5, outer = TRUE)
```



## Question 11.4
Find the intersection points $A(k)$ in $(0,\sqrt{k})$ of the curves
$$S_{k-1}(a)=P(t(k-1)>\sqrt{\frac{a^2(k-1)}{k-a^2}})$$
and $$S_k(a)=P(t(k)>\sqrt{\frac{a^2k}{k+1-a^2}})$$
for$ k = 4:25,100,500,1000,$where $t(k)$ is a Student t random variable with k degrees of freedom. (These intersection points determine the critical values for a t-test for scale-mixture errors proposed by Sz´ekely [260].)

## amswer

Subtract two equations,let is equals to 0.

Let's first look at how many roots there are in each interval:
```{r}
tF<-function(x,n){
   exp(lgamma((n+1)/2)-lgamma(n/2))*(1+x^2/n)^(-(n+1)/2)/(sqrt(n*pi))
}
KKK<-c(4:25,100,500,1000)
#par(mfrow=c(5,length(KK)/5))
for (dd in 1:5){
  KK<-KKK[(5*(dd-1)+1):(5*dd)]
  #par(mfrow=c(1,5))
  for (k in  1:length(KK)){
   kk<-KK[k]
   l<-0.01
   x<-seq(0,sqrt(kk)-l,by=l)
   y<-numeric(length(x))
   for (p in 1:length(x)){
      pp<-x[p]
      l1<-sqrt(pp^2*(kk-1)/(kk-pp^2))
      l2<-sqrt(pp^2*kk/(kk+1-pp^2))
      la<-integrate(tF,lower=l1,upper=Inf,n=(kk-1))
      lb<-integrate(tF,lower=l2,upper=Inf,n=kk)
      y[p]<-la[1]$value-lb[1]$value
   }
   plot(x,y,'l',xlim=c(0,sqrt(kk)),ylim=c(min(y)*1.2,max(y)*1.2),xlab=paste("The corresponding value of k is",kk))
   abline(h=0,lty=3)
  }
}
```

It can be seen that there is only one root in each of the corresponding 25 intervals, and within the interval, the following dichotomy method is adopted to find the root:

```{r}
tF<-function(x,n){
   exp(lgamma((n+1)/2)-lgamma(n/2))*(1+x^2/n)^(-(n+1)/2)/(sqrt(n*pi))
}
KKK<-c(4:25,100,500,1000) 
tt<-numeric(length(KKK))#Storage root
n<-numeric(length(KKK))
for (k in  1:length(KKK)){
   kk<-KKK[k]
   l<-0.01
   x1<-l#The left endpoint of the interval where the root is
   x2<-sqrt(kk)-l#The right endpoint of the interval where the root is
   barr<-10^(-6)#The threshold value
   y<-numeric(2)
   
   while (abs(x1-x2)>barr){
      tt[k]<-(x1+x2)/2#The midpoint of the interval
      
      al<-sqrt((x1)^2*(kk-1)/(kk-(x1)^2))
      bl<-sqrt((x1)^2*kk/(kk+1-(x1)^2))
      
      cl<-sqrt((tt[k])^2*(kk-1)/(kk-(tt[k])^2))
      dl<-sqrt((tt[k])^2*kk/(kk+1-(tt[k])^2))
      
      ala<-integrate(tF,lower=al,upper=Inf,n=(kk-1))
      blb<-integrate(tF,lower=bl,upper=Inf,n=kk)
      
      cla<-integrate(tF,lower=cl,upper=Inf,n=(kk-1))
      dlb<-integrate(tF,lower=dl,upper=Inf,n=kk)
      y[1]<-ala[1]$value-blb[1]$value
      y[2]<-cla[1]$value-dlb[1]$value
      if(y[1]*y[2]<=0){
         x2<-tt[k]
      }
      else{
         x1<-tt[k] 
      }
      n[k]<-n[k]+1
   }     
}

tty<-numeric(length(tt))


for (dd in 1:5){
   l<-5*(dd-1)+1
   KK<-KKK[l:(l+4)]
   #par(mfrow=c(1,5))
   for (k in  1:length(KK)){
      kk<-KK[k]
      ll<-0.01
      xx<-seq(0,sqrt(kk)-ll,by=ll)
      yy<-numeric(length(xx))
      for (p in 1:length(xx)){
         pp<-xx[p]
         l1<-sqrt(pp^2*(kk-1)/(kk-pp^2))
         l2<-sqrt(pp^2*kk/(kk+1-pp^2))
         la<-integrate(tF,lower=l1,upper=Inf,n=(kk-1))
         lb<-integrate(tF,lower=l2,upper=Inf,n=kk)
         yy[p]<-la[1]$value-lb[1]$value
      }
      
      tt1<-sqrt(tt[l+k-1]^2*(kk-1)/(kk-tt[l+k-1]^2))
      tt2<-sqrt(tt[l+k-1]^2*kk/(kk+1-tt[l+k-1]^2))
      tta<-integrate(tF,lower=tt1,upper=Inf,n=(kk-1))
      ttb<-integrate(tF,lower=tt2,upper=Inf,n=kk)
      tty[l+k-1]<-tta[1]$value-ttb[1]$value
      
      plot(xx,yy,'l',xlab="",xlim=c(0,sqrt(kk)),ylim=c(min(yy)*1.2,max(yy)*1.2))
      abline(h=0,lty=3)
      par(new=TRUE)
      plot(tt[l+k-1],tty[l+k-1],'b',col='red',xlab=paste("The corresponding k value is:",kk),ylab="",xlim=c(0,sqrt(kk)),ylim=c(min(yy)*1.2,max(yy)*1.2))
   }
    if(dd==1){
    mtext("The location of each root (red circle)", side =3, line = -2.5, outer = TRUE)
     }
}

for (ind in 1:length(tt)){
   print(paste('when k=',KKK[ind],',the corresponding root is:',tt[ind]))
}
```

## Question EM algorithm

## answer

The likelihood function for the complete data is $L=(p^2)^{n_{AA}}(q^2)^{n_{BB}}(r^2)^{n_{OO}}(2pr)^{n_{AO}}(2qr)^{n_{BO}}(2pq)^{n_{AB}}$
and know that under the observation data,$n_{AA}\sim B(n_A.,\frac{p^2}{p^2+2pr}),n_{BB}\sim B(n_B.,\frac{q^2}{q^2+2qr}),$

for $\forall i>0$ let $M=n_{A.}+n_{AB}+n_{A.}\frac{p_i^2}{p_i^2+2p_ir_i},N=n_{B.}+n_{AB}+n_{B.}\frac{q_i^2}{q_i^2+2q_ir_i},$
$S=2n_{OO}+n_{A.}+n_{B.}-n_{B.}\frac{q_i^2}{q_i^2+2q_ir_i}-n_{A.}\frac{p_i^2}{p_i^2+2p_ir_i}$,therefore, it can be deduced according to EM algorithm,$p_{i+1}=\frac{M}{M+N+S},q_{i+1}=\frac{N}{M+N+S},r_{i+1}=1-p_{i+1}-q_{i+1}=\frac{S}{M+N+S}$

The logarithmic likelihood function of the observed data is $LL=n_{A.}ln(p^2+2pr)+n_{B.}ln(q^2+2qr)+2n_{OO}ln(r)+n_{AB}ln(2pq)=444ln(p^2+2pr)+132ln(q^2+2qr)+722ln(r)+63ln(2pq)$

Set the initial value :$p_0=0.1=q_0,r_0=0.8$,the programming and results are as follows:

```{r}
n<-15
L<-matrix(rep(0,n*3),nrow=n)#Three parameters
R<-numeric(n)#Maximum likelihood for observed data
L[1,1]<-L[1,2]<-0.1
L[1,3]<-0.8
bar<-1e-8
k=1
Ba<-sqrt((L[k,1])^2+(L[k,1])^2+(L[k,1])^2)
while(Ba>bar){
  p<-L[k,1]
  q<-L[k,2]
  r<-L[k,3]
  R[k]<-444*log(p^2+2*p*r)+132*log(q^2+2*q*r)+722*log(r)+63*log(2*p*q)
  M<-444+63+444*p^2/(p^2+2*p*r)
  N<-132+63+132*q^2/(q^2+2*q*r)
  S<-2*361+444+132-132*q^2/(q^2+2*q*r)-444*p^2/(p^2+2*p*r)
  L[k+1,1]<-M/(M+N+S)
  L[k+1,2]<-N/(M+N+S)
  L[k+1,3]<-S/(M+N+S)
  Ba<-sqrt((L[k+1,1]-L[k,1])^2+(L[k+1,2]-L[k,2])^2+(L[k+1,3]-L[k,3])^2)
  k<-k+1
}

print("The three parameter values (p,q,r) estimated each time are respectively:")
print(L[1:(k-1),])
print("The logarithmic likelihood function value of the observed data is as follows:")

plot(R[1:(k-1)],type='b',col='red',xlab="Number of calculations",ylab="The logarithmic likelihood function value of the observed data")
```

It can be seen that the corresponding values of the three parameters should be 0.3, 0.1, 0.6, and the logarithmic likelihood function value corresponding to the observed data is increasing



## Exercise 3

Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:


formulas <- list(

mpg ~ disp,

mpg ~ I(1 / disp),

mpg ~ disp + wt,

mpg ~ I(1 / disp) + wt

)

## answer

The code and the corresponding two programming results are as follows:

```{r}
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)

# lapply
L1<- lapply(formulas, lm, data = mtcars)
L2<- lapply(formulas, function(x) lm(formula = x, data = mtcars))

# circulation
L3 <- vector("list", length(formulas))
n<-seq_along(formulas)
for (i in n){
  L3[[i]] <- lm(formulas[[i]], data = mtcars)
}

print("The results of lapply:")
print(L1)
print("The results of loop:")
print(L3)
```

The result is as above.

## Exercise 3

The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.

trials <- replicate(

100,

t.test(rpois(10, 10), rpois(7, 10)),

simplify = FALSE

)

Extra challenge: get rid of the anonymous function by using [[ directly.

## answer

The programming and results with  the $sapply()$ and without anonymous functions are as follows:

```{r}
trials <- replicate(
  100, 
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)

# Using sapply () and anonymous functions:
sapply(trials, function(x) x[["p.value"]])
# Do not use anonymous functions:
sapply(trials, "[[", "p.value")
```


## Exercise 6

Implement a combination of Map() and vapply() to create an lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?

## answer
The data sets $iris,mtcars,cars$are used

The program is as follows:

```{r}
testlist <- list(iris, mtcars, cars)

lapply(testlist, function(x) vapply(x, mean, numeric(1)))

lmapply <- function(X, FUN, FUN.VALUE, simplify = FALSE){
  out <- Map(function(x) vapply(x, FUN, FUN.VALUE), X)
  if(simplify == TRUE){return(simplify2array(out))}
  out
}

lmapply(testlist, mean, numeric(1))
```
The parameters taken by the function are showed in the  programming function lmapply  above.


## Question 

 1.1 Write an Rcpp function for Exercise 9.4 (page 277, Statistical
Computing with R).

1.2 Compare the corresponding generated random numbers with
those by the R function you wrote before using the function
“qqplot”.

 1.3 Compare the computation time of the two functions with the
function “microbenchmark”.

1.4 Comments your results.

## answer

The code is as follows:

```{r}
library(Rcpp)
dir_cpp <- '../src/'
#Can create source file in Rstudio
sourceCpp(paste0(dir_cpp,"Metropolis.cpp"))
#library(StatComp20091)
library(microbenchmark)

LSR<-function(x){
  f<-exp(-abs(x))/2
  return(f)
}

rw.Metropolis<-function(sigma,x0,N){
  x<-numeric(N)
  x[1]<-x0
  u<-runif(N)
  k<-0
  for (i in 2:N){
    y<-rnorm(1,x[i-1],sigma)
    if (u[i] <= LSR(y) / LSR(x[i-1]))
    {x[i] <- y}
    else
    {x[i] <- x[i-1]
    k<-k+1
    }
  }
  return(list(x=x,k=k))
}


N<-5000
sigma<-c(0.5,1,5,10)
x0<-10
lk<-matrix(rep(0,length(sigma)*2),ncol=2)

for ( i in c(1,2,3,4)){
  ts<-microbenchmark(rw1<-rw.Metropolis(sigma[i],x0,N),rw2<-Metropolis(sigma[i],x0,N))
 
  

  #par(mfrow = c(1, 2))
  plot(rw1$x,type='l',col='red',xlab=paste('R:',expression(sigma),'=',sigma[i])) 
  plot(rw2[,1],type='l',col='blue',xlab=paste('C++:',expression(sigma),'=',sigma[i]))
  
  mtext("The performance of different language ", side =3, line = -2.5, outer = TRUE)
   print(paste('when sigma=',sigma[i],',the running time of the corresponding two cases is:'))
  RS<-summary(ts)[,c(1,3,5,6)]
  print(RS)
  lk[i,]<-(c(N-(rw1$k),N-rw2[1,3]))/N#Accept rate

  print(paste('The normal standard deviation of the first chain in R is:',sigma[i],',accept rate is :',lk[i,1],';'))
  print(paste('The normal standard deviation of the first chain in C++:',sigma[i],',accept rate is :',lk[i,2]))
  
}
```
It can be seen from the four cases that the computing time is basically more than 20 times of that of C++.